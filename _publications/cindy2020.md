---
title: "Continual Learning Long Short Term Memory"
collection: publications
permalink: /publication/cindy2020
excerpt: 'This is one of the pioneers in continual learning for NLP related works via recurrent neural networks.'
date: 2020-09
author: Xin Guo, Yu Tian, Qinghan Xue, Panos Lampropoulos, Steven Eliuk, Kenneth Barner and Xiaolong Wang*
venue: 'EMNLP(findings)'
#paperurl: 'https://arxiv.org/pdf/1708.04728'
#videourl: 'https://www.youtube.com/watch?v=KjdkchqlQOU&feature=youtu.be'
#citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
#[Download paper here](https://arxiv.org/pdf/1708.04728)
---
Catastrophic forgetting in neural networks indicates the performance decreasing of deep learning models on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell in Recurrent Neural Network (RNN) in this paper. CL-LSTM considers not only the state of each individual task's output gates but also the correlation of the states between tasks, so that the deep learning models can incrementally learn new tasks without catastrophically forgetting previously tasks. Experimental results demonstrate significant improvements of \ourApproach~over state-of-the-art approaches on spoken language understanding (SLU) tasks.

